{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "immhydXaPicM"
   },
   "source": [
    "# 컨텐츠 기반 영화 추천 시스템 구축\n",
    "\n",
    "- 팀 이름: MM-Up\n",
    "- 팀 구성원: 송현달, 유병욱, 지영석\n",
    "<br />\n",
    "\n",
    "- 프로젝트 간략 개요: 사용자가 한글로 영화 제목을 검색하면, 유사도를 기반으로 연관성을 갖는 영화 상위 10개를 추천해주는 시스템을 구축했음. 또한 연도별 정렬, 장르 기반 추천, 키워드 기반 추천 기능 모두 구현했음.\n",
    "\n",
    "```\n",
    "프로젝트 프로세스 구성!\n",
    "\n",
    "1. 데이터 스크래핑 (유병욱) - BS4와 requests모듈을 사용한 영화 키워드, 제목, 장르 크롤링했음.\n",
    "\n",
    "2. 데이터 전처리 (송현달) - 수집한 데이터를 바탕으로 특수 문자 제거-영화 출시 연도 컬럼 생성-불필요한 문자열 제거 등의 전처리를 진행했음.\n",
    "\n",
    "3. 모델링 (지영석) - 전처리 완료된 데이터를 바탕으로, 컨텐츠 기반 필터링 알고리즘을 사용해서 추천시스템을 구축했음.\n",
    "\n",
    "4. 시각화 (송현달) - 영화 장르 키워드 컬럼을 바탕으로, 워드 클라우드 시각화 구축\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBGyuoJOSDD1"
   },
   "source": [
    "# 1. 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n-4ZywxPXdt"
   },
   "outputs": [],
   "source": [
    "# 크롤링 하기위한 import선언\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# 크롤링 후 csv파일로 저장하기 위한 import\n",
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "# 나중에 페이지가 늘어날 것을 대비하여 만든 것, 마지막 페이지의 수를 int변수로 저장한다.\n",
    "page_response = requests.get('https://moviekeyword.com')\n",
    "page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "last_page = page_soup.select_one('#main > div > nav > div > a:nth-child(8)')\n",
    "last = last_page.get_text().replace(',','')\n",
    "int_last = int(last)\n",
    "\n",
    "# start_url뒤에 페이지 수를 추가해주면 그 페이지로 이동한다.\n",
    "start_url = 'https://moviekeyword.com/page/'\n",
    "\n",
    "def page_jump(n):\n",
    "    movies_data = []\n",
    "    for i in range(50*(n) + 1, 50*(n+1)+1): #뒤의 if문과 연관시켜서 값을 넣으면 된다.\n",
    "        page_num = i\n",
    "        URL = start_url + str(page_num)\n",
    "        response = requests.get(URL)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 여기까지 페이지 관련으로 for문을 받아 한바퀴를 돌때마다, 다음 페이지로 이동\n",
    "\n",
    "        # title 추출 시작\n",
    "        movies_page_list = soup.select('div[id=page] > div[id=content-area] > div[id=content] > div[id=primary] > main[id=main] > div[id=primary] > main[id=main] > article')\n",
    "\n",
    "        for movie_page in movies_page_list:\n",
    "            a_tag = movie_page.select_one('div.post-summary-container.col-md-11 > header > h2 > a')\n",
    "            movies_title = a_tag.get_text() # title 추출\n",
    "            movies_link = a_tag['href'] # 각 페이지에 접속하여 content와 keyword를 추출하기 위하여 주소 추출\n",
    "\n",
    "            # 여기까지 title 추출하기\n",
    "\n",
    "            # 추출한 링크로 이동하기 위한 response를  mv_response로 설정\n",
    "            mv_response = requests.get(movies_link)\n",
    "            new_soup = BeautifulSoup(mv_response.text, 'html.parser')\n",
    "\n",
    "            # 중간에 content가 5번 위치일 경우 사용할 경로\n",
    "            movie_content = new_soup.select_one('#right-sidebar > div.right-sidebar-meta-container > ul > li:nth-child(5) > div.meta-value-container > span.meta-value > a')\n",
    "            # 5번 위치가 아니라 4번에 있는 경우 사용할 경로\n",
    "            movie_content_reserve = new_soup.select_one('#right-sidebar > div.right-sidebar-meta-container > ul > li:nth-child(4) > div.meta-value-container > span.meta-value > a')\n",
    "            \n",
    "            #  위에서 설명한데로 5번이 비어있는경우 4번에서 값을 가지고 온다는 if문\n",
    "            if movie_content != None:\n",
    "                content = movie_content.get_text()\n",
    "            elif movie_content_reserve != None:\n",
    "                content = movie_content_reserve.get_text()\n",
    "            else:\n",
    "                content = '없음'\n",
    "            \n",
    "            # 여기까지 장르(content) 추출하기\n",
    "\n",
    "            movie_keywords = new_soup.select('#right-sidebar > div.right-sidebar-meta-container > ul > li:nth-last-child(1) > div.meta-value-container > span.meta-value')\n",
    "            \n",
    "            keywords = []\n",
    "            # for 문으로 keyword 값들을 각각 가지고 와서 keywords에 append시킨다.\n",
    "            for movie_keword in movie_keywords:\n",
    "                keyword_tag = movie_keword.select_one('a')\n",
    "                # if문은 keyword가 없는 경우 키워드가 없다는 것을 출력시키기 위한 것\n",
    "                if keyword_tag != None: \n",
    "                    keyword = keyword_tag.get_text().strip()\n",
    "                else:\n",
    "                    keyword = '없음'\n",
    "                keywords.append(keyword)\n",
    "\n",
    "            # 여기까지 키워드 추출하기\n",
    "\n",
    "            movie_data = {\n",
    "                'title': movies_title,\n",
    "                'content': content,\n",
    "                'keyword': keywords\n",
    "            }\n",
    "            movies_data.append(movie_data)\n",
    "            # print(movie_data) # 영화 제목, 장르, 키워드 출력\n",
    "            # print(movies_title) #영화 제목 출력\n",
    "            # print(content) # 영화 장르 출력\n",
    "            # print(keywords) # 영화 키워드 출력\n",
    "            print(i, \" \", movie_data) \n",
    "            # 에러값의 원인을 찾기위해 페이지 값과 영화데이터 값을 각각 출력\n",
    "            # 일단 출력이 되었으면 거기까지는 문제가 없다는 뜻이다.\n",
    "\n",
    "        # 위의 첫 for문과 연계 너무 많이하면 오류발생되서 데이터가 하나도 저장이 안됨\n",
    "        # if문을 사용하여 20페이지 당 파일 이름을 바꾸어서 데이터를 새로 저장\n",
    "        if i % 50 == 0 or i == int_last:\n",
    "            dataframe = pd.DataFrame(movies_data)\n",
    "            dataframe.to_csv(f\"./save_data_{n}.csv\",encoding='UTF-16',header=False,index=False)\n",
    "\n",
    "n = 0 # 해당 폴더에서 출력되지 않은 숫자를 입력하면 그것부터 다시 출력시킨다.\n",
    "       \n",
    "for a in range(n+1,(int_last//50)+2):\n",
    "    page_jump(n)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIJSdqTeSFB7"
   },
   "source": [
    "# 2. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bc6EGf9_SG9j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "# csv파일 편집을 위해 pandas 호출\n",
    "# 테이블을 호출했을때 가독성을 높이기 위한 tabulate 호출\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/admin/AI/miniproject/movie_data_before.csv\", encoding = 'utf-8') \n",
    "# pandas로 csv파일을 읽어준다.(csv경로 재설정 확인 필수)\n",
    "\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='psql'))\n",
    "print(tabulate(data.tail(), headers='keys', tablefmt='psql'))\n",
    "# 제대로 불러왔는지 확인한다.\n",
    "print(data.shape)\n",
    "# 데이터프레임의 차원을 확인한다.\n",
    "print(data.dtypes)\n",
    "# 컬럼의 타입을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqkSVAuySPzD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "year = data['title']\n",
    "# year에 title컬럼값을 저장한다.\n",
    "year = year.str.replace(pat=r'(.*?)\\,\\s', repl=r'', regex=True)\n",
    "# year에서 ', '를 포함한 이전 문자열 모두 제거한다.\n",
    "year = year.str.replace(pat=r'(.*?)\\(', repl=r'', regex=True)\n",
    "# year에서 '('를 포함한 이전 문자열 모두 제거한다.\n",
    "year = year.str.replace(pat=r'[\\D]', repl=r'', regex=True)\n",
    "# year에서 숫자를 제외한 모든 문자열을 제거한다.\n",
    "year = year.replace(r'', r'2010', regex=True)\n",
    "# year에서 완전히 공백인 공간은 '2010'으로 대체한다.\n",
    "\n",
    "print(year)\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPnA8mwSSUur",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data,columns=['title', 'year', 'genres', 'keywords'])\n",
    "# 데이터프레임에 'year'컬럼을 추가해준다.\n",
    "data['year'] = year\n",
    "# 'year'컬럼에 year값을 저장한다.\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='psql'))\n",
    "print(tabulate(data.tail(), headers='keys', tablefmt='psql'))\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqLlxVscSV-k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[data['year'].str.len() != 4]\n",
    "# 'year'컬럼값이 길이4가 아닌 행을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPVwHXq3SZE0"
   },
   "outputs": [],
   "source": [
    "for row in range(len(data.year)):\n",
    "    data['year'] = data['year'].str[-4:]\n",
    "    #'year'컬럼값을 뒤에서 길이4로 슬라이싱한다.\n",
    "\n",
    "data = data[data['year'].str.len() == 4]\n",
    "# 'year'컬럼값이 길이4가 아닌 행을 제거한다.\n",
    "\n",
    "data[data['year'].str.len() != 4]\n",
    "# 'year'컬럼값이 길이4가 아닌 행을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bf90-u-2SfMk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['title'] = data['title'].str.replace(pat=r'\\((.*?)\\)', repl=r' ', regex=True)\n",
    "# 'title'컬럼에서 소괄호와 그 사이의 문자를 모두 공백으로 바꾼다.\n",
    "\n",
    "print(data['title'])\n",
    "# 처리된 'title'컬럼을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-uV4vD6Shk8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['keywords'] = data['keywords'].str.replace(pat=r'[^\\w\\s]', repl=r' ', regex=True)\n",
    "# 'keywords'컬럼에서 공백, 알파벳, 숫자가 아닌 문자를 모두 공백으로 바꾼다.\n",
    "data = data[data['keywords'] != '  없음  ']\n",
    "# 'keywords'컬럼이 '  없음  '으로 채워진 행을 제거한다.\n",
    "# (원래 ['없음']이라고 적혀있는 값에서 특수문자를 모두 공백으로 바꿨으므로\n",
    "# 양끝에 공백을 2개씩 넣어줘야 정상적으로 인식한다.)\n",
    "\n",
    "print(data['keywords'])\n",
    "# 처리된 'keywords'컬럼을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUb92XAGSirM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "# 데이터 프레임의 인덱스를 리셋해주고 중간중간 빈 인덱스를 컬럼으로 빼준다.\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='psql'))\n",
    "print(tabulate(data.tail(), headers='keys', tablefmt='psql'))\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxRjC4QHSkFM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.drop(['index'], axis='columns', inplace=True)\n",
    "# 'index'컬럼으로 빼낸 값을 제거한다.\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='psql'))\n",
    "print(tabulate(data.tail(), headers='keys', tablefmt='psql'))\n",
    "print(data.shape)\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2uP82gdSn_s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['year'] = data['year'].astype(str).astype(int)\n",
    "# 정렬을 용이하게 하기위해서 'year'컬럼을 정수형으로 변환해준다.\n",
    "print(data.dtypes)\n",
    "# 컬럼의 타입을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6M9Sx4zgSqKM"
   },
   "outputs": [],
   "source": [
    "data.to_csv('movie_data_after.csv')\n",
    "# 결과물을 'movie_data_after.csv'파일로 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XB4-EM_TSr8k"
   },
   "source": [
    "# 3. 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1arwkFN2Stn8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def tf_idf_matrics(column):  #tf-idf사용해서 문장-단어 간 유사도 측정 함수\n",
    "    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "    tfidf_mat = tfidf_vec.fit_transform(column.values.astype(\"U\"))\n",
    "    return tfidf_mat\n",
    "\n",
    "# tf_idf_mat = tf_idf_matrics(df[\"keywords\"])\n",
    "# print(tf_idf_mat)\n",
    "\n",
    "def cosine_sim_matrics(tfidf_mat): #코사인 유사도 측정 함수\n",
    "    cos_sim = cosine_similarity(tfidf_mat, tfidf_mat)\n",
    "    sim_idx = np.argsort(-cos_sim)\n",
    "    return sim_idx\n",
    "\n",
    "# genres_sim, sim_idx = cosine_sim_matrics(tf_idf_mat)\n",
    "# print(genres_sim, sim_idx)\n",
    "\n",
    "def recommandation_mvs_top10(inputs_mv, sim_idx, df): #input을 사용자로부터 받고, 유사도가 제일 높은 10개의 영화를 추천해주는 함수\n",
    "#검색한 영화의 인덱스를 찾아준다.\n",
    "    mv_idx = df[df.title==inputs_mv.lstrip().rstrip()].index.values\n",
    "    print(mv_idx)\n",
    "    \n",
    "#인덱스 값을 기준으로 유사도가 높은 영화 인덱스를 출력한다.\n",
    "    sim_mv = sim_idx[mv_idx, :int(10)]\n",
    "    print(sim_mv)\n",
    "\n",
    "    sim_mv_idx = sim_mv.reshape(-1) #2차원 벡터 -> 1차원으로 변경\n",
    "    print(sim_mv_idx)\n",
    "    #x = df.iloc[sim_mv_idx]\n",
    "    \n",
    "    top_10_title = pd.Series(df.iloc[sim_mv_idx, 0])\n",
    "    top_10_years = pd.Series(df.iloc[sim_mv_idx, 1])\n",
    "    top_10_genres = pd.Series(df.iloc[sim_mv_idx, 2])\n",
    "    top_10_keywords = pd.Series(df.iloc[sim_mv_idx, 3])\n",
    "\n",
    "    recommandation_df = pd.DataFrame({\n",
    "        \"title\": top_10_title,\n",
    "        \"year\": top_10_years,\n",
    "        \"genres\": top_10_genres,\n",
    "        \"keywords\": top_10_keywords\n",
    "    })\n",
    "    \n",
    "    return recommandation_df\n",
    "\n",
    "# inputs_mv = input()\n",
    "# r = recommandation_mvs_top10(inputs_mv, sim_idx)\n",
    "# print(r)\n",
    "\n",
    "# def main1():\n",
    "#     drive.mount(\"/content/gdrive\")\n",
    "#     df = pd.read_csv(\"/content/gdrive/My Drive/광인사 자연어 프로젝트/final_prepro_data2.csv\")\n",
    "#     tf_idf_mat = tf_idf_matrics(df[\"keywords\"])\n",
    "#     print(f'TF-IDF-VECTOR- \\n {tf_idf_mat}')\n",
    "#     genres_sim, sim_idx = cosine_sim_matrics(tf_idf_mat)\n",
    "#     print(f\"\\n COSINE-VECTOR- \\n {genres_sim}\")\n",
    "#     print(f\"\\n SORTED-COSINE-VECTOR- \\n {genres_sim}\")\n",
    "#     inputs_mv = input()\n",
    "#     r = recommandation_mvs_top10(inputs_mv, sim_idx)\n",
    "#     print(r)\n",
    "\n",
    "def main2():\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    #data = pd.read_csv(\"/content/gdrive/My Drive/mvs_final_csvfile.csv\")\n",
    "    data = pd.read_csv(\"recomm_data.csv\")\n",
    "    df = data.drop(\"Unnamed: 0\", axis=1)\n",
    "    inputs_mv = input(\"---찾고자 하는 영화를 입력해주세요!--- \\n\")\n",
    "    input_method_of_recommandation = int(input('''\n",
    "                    검색하시는 영화 이외에, 회원님이 좋아하실만한 영화 10개를 더 추천해드리려고 합니다! \\n\n",
    "                    1. 비슷하거나 같은 장르의 영화를 추천받고 싶으신가요? \\n\n",
    "                    2. 비슷하거나 같은 키워드나 주제를 가진 영화를 추천받고 싶으신가요? \\n\n",
    "                            '''))\n",
    "    \n",
    "    if input_method_of_recommandation == 1:\n",
    "        tf_idf_mat = tf_idf_matrics(df[\"genres\"])\n",
    "    elif input_method_of_recommandation == 2:\n",
    "        tf_idf_mat = tf_idf_matrics(df[\"keywords\"])\n",
    "    else:\n",
    "        print(\"다시 선택해주세요!\")\n",
    "\n",
    "    #print(f'TF-IDF-VECTOR- \\n {tf_idf_mat}')\n",
    "    sim_idx = cosine_sim_matrics(tf_idf_mat)\n",
    "    #print(f\"\\n COSINE-VECTOR- \\n {genres_sim}\")\n",
    "    #print(f\"\\n SORTED-COSINE-VECTOR- \\n {genres_sim}\")\n",
    "    print(\"---처리 중 입니다!--\")\n",
    "    recomm_df = recommandation_mvs_top10(inputs_mv, sim_idx, df)\n",
    "    recomm_df = recomm_df.sort_values(by=\"year\", ascending=False)\n",
    "    recomm_df.to_csv(\"recommand_top_10_mvs.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "# 한글로 이루어진 키워드를 처리하기 위해서 konlpy의 Twitter를 호출해준다.\n",
    "# 키워드의 빈도수를 측정하기 위해 collections의 Counter를 호출해준다.\n",
    "# csv파일을 편집하기위해 pandas를 호출해준다.\n",
    "\n",
    "file = pd.read_csv(\"C:/Users/admin/AI/miniproject/movie_data_after.csv\", encoding = 'utf-8')\n",
    "# csv파일을 호출한다.\n",
    "kw_file = file.keywords\n",
    "# 'keywords'컬럼만 따로 불러온다.\n",
    "print(kw_file)\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_file = kw_file.str.strip()\n",
    "# 컬럼값 양쪽의 공백을 제거한다.\n",
    "kw_file = kw_file.str.replace(pat=r'\\s+', repl=r' ', regex=True)\n",
    "# 컬럼값 가운데 중복된 공백을 한개로 줄인다.\n",
    "print(kw_file)\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_file = kw_file.to_list()\n",
    "# 시리즈 형식인 kw_file을 리스트화한다.\n",
    "kw_file = ' '.join(kw_file)\n",
    "# 리스트가 된 kw_file을 str으로 변환하여 ' '을 기준으로 split한다. \n",
    "kw_file = kw_file.split(' ')\n",
    "# str인 kw_file을 리스트로 변환하여 ' '을 기준으로 split한다.\n",
    "print(kw_file)\n",
    "# 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(kw_file)\n",
    "# kw_file내 단어들의 빈도수를 계산한다.\n",
    "words = dict(count.most_common())\n",
    "# 빈도수를 계산한 값에서 가장 빈도수가 많은 순으로 정렬하여 dictionary 형태로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rc('font',family = 'Malgun Gothic')\n",
    "# 워드 클라우드에 필요한 함수들을 호출한다.\n",
    "# font는 Malgun Gothic\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    font_path = 'C:/Windows/Fonts/malgun.ttf',\n",
    "    background_color='white',\n",
    "    colormap = \"Accent_r\",\n",
    "     width=1500,\n",
    "     height=1000\n",
    "     ).generate_from_frequencies(words)\n",
    "# font_path에 한글 폰트가 있는 경로를 적어준다. \n",
    "# background_color는 'white'로 설정한다.\n",
    "# 마지막 괄호에 dictionary 형태로 만들었던 words를 넣어준다. \n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO12hCU+MzIZdff0qS/GX7i",
   "collapsed_sections": [],
   "name": "Total_Source_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
